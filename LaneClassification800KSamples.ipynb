{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data - Subaru + VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features numpy (8, 800000)\n",
      "Test Targets numpy (8,)\n",
      "Data points chosen for test: [4, 7, 14, 17, 25, 21, 36, 33]\n"
     ]
    }
   ],
   "source": [
    "#DATASET PREPERATION\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "data = sio.loadmat('../accelane_mat_v3/SBSB_6Khz_preprocessed_AccZ.mat')\n",
    "start_no = 0\n",
    "range_len = 800000\n",
    "test_set = np.zeros((1,range_len))\n",
    "test_datapoints = [4,7,14,17,25,21,36,33]\n",
    "num_datapoints = 20\n",
    "labels = []\n",
    "for i in range (num_datapoints):     \n",
    "    if i < 10:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "    varName = \"accZ_L{}T{}_p\".format(2 if i < 10 else 3, (i%10)+1)\n",
    "    curr = np.asarray(data[varName][start_no : start_no+range_len]).reshape(1,range_len)\n",
    "    if i in test_datapoints:\n",
    "        test_set = np.append(test_set, curr, axis = 0)\n",
    "data = sio.loadmat('../accelane_mat_v3/VWVW_preprocessed_AccZ.mat')\n",
    "for i in range (num_datapoints):\n",
    "    if i < 10:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "    varName = \"accZ_L{}T{}_p\".format(2 if i < 10 else 3, (i%10)+1)\n",
    "    curr = np.asarray(data[varName][start_no : start_no+range_len]).reshape(1,range_len)\n",
    "    if i in test_datapoints:\n",
    "        test_set = np.append(test_set, curr, axis = 0)\n",
    "\n",
    "test_set = np.delete(test_set, (0), axis=0)\n",
    "training_size = (num_datapoints*2) - len(test_datapoints)\n",
    "test_labels = []\n",
    "for i in np.arange(len(labels)):\n",
    "    if i in test_datapoints:\n",
    "        test_labels.append(labels[i])\n",
    "test_labels = np.asarray(test_labels).reshape(len(test_datapoints),1)\n",
    "test_set = np.append(test_set, test_labels, axis = 1)\n",
    "test_targets_numpy = test_set[:,-1]\n",
    "test_features_numpy = test_set[:,:-1]\n",
    "print(\"Test Features numpy {}\".format(test_features_numpy.shape))\n",
    "print(\"Test Targets numpy {}\".format(test_targets_numpy.shape))\n",
    "print(\"Data points chosen for test: {}\".format(test_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples In Storage: Features (296952, 800000), Labels (371712, 1)\n"
     ]
    }
   ],
   "source": [
    "import tables\n",
    "train_features_numpy_filename = \"data/TrainFeatures800K.h5\"\n",
    "# train_features_second_numpy_filename = \"data/TrainFeatures800K2.h5\"\n",
    "train_targets_numpy_filename = \"data/TrainLabels800K.h5\"\n",
    "\n",
    "try:\n",
    "    train_features_file = tables.open_file(train_features_numpy_filename, mode='r')\n",
    "#     train_features_second_file = tables.open_file(train_features_second_numpy_filename, mode='r')\n",
    "    train_targets_file = tables.open_file(train_targets_numpy_filename, mode='r')\n",
    "    train_features_file.root.data.truncate(296952)\n",
    "\n",
    "    print(\"Num Samples In Storage: Features {}, Labels {}\".format(train_features_file.root.data.shape, train_targets_file.root.data.shape))\n",
    "    np.random.seed(10)\n",
    "    index_list = np.random.randint(296952,size=500)\n",
    "    train_len = int(len(index_list) * 0.8)\n",
    "    train_features_numpy_tr = (train_features_file.root.data[i] for i in index_list[:train_len])\n",
    "    train_targets_numpy_tr = (train_targets_file.root.data[i] for i in index_list[:train_len])\n",
    "    train_features_numpy_val = (train_features_file.root.data[i] for i in index_list[train_len:])\n",
    "    train_targets_numpy_val = (train_targets_file.root.data[i] for i in index_list[train_len:])\n",
    "finally:\n",
    "    pass\n",
    "#     train_features_file.close()\n",
    "#     train_targets_file.close()\n",
    "#     train_features_second_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) for a given epoch\n",
    "    take a drive\n",
    "    split them into 20k lengths based on stride\n",
    "    batch these values together\n",
    "    train the lstm on this batch\n",
    "    reset state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size for model:  1\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "output_dim = 2\n",
    "hidden_dim = 2#200\n",
    "num_classes = 2\n",
    "input_dim = 100000\n",
    "seq_length = 8\n",
    "num_epochs = 1\n",
    "embedding_dim = 991#491\n",
    "lr = 0.01\n",
    "stride = input_dim\n",
    "batch_size = int((range_len-(seq_length*input_dim))/stride + 1)\n",
    "print(\"batch_size for model: \", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoBatches:\n",
    "    def __init__(self, length, stride, signal):\n",
    "        self.length = length\n",
    "        self.stride = stride\n",
    "        self.signal = signal\n",
    "        self.curr_index = 0\n",
    "    def getNextBatch(self):\n",
    "        if (self.curr_index >= len(self.signal)):\n",
    "            return\n",
    "        upper_lim = min(self.curr_index + self.length, len(self.signal))\n",
    "        to_ret =  self.signal[self.curr_index:upper_lim, 0]\n",
    "        self.curr_index += self.stride\n",
    "        done = False\n",
    "        if (self.length > len(to_ret)):\n",
    "            to_ret = np.pad(to_ret, (0,self.length - len(to_ret)), 'constant', constant_values=(0))\n",
    "            done = True\n",
    "        return to_ret.reshape(self.length, 1), done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, seq_length, hidden_dim, batch_size, output_dim, num_layers = 1):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.seq_length = seq_length \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.batch_size = batch_size \n",
    "        self.num_layers = num_layers \n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        self.embedding = nn.Sequential( # 10,000\n",
    "            \n",
    "            nn.AvgPool1d(1000, stride = 100)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.GRU(input_size = self.embedding_dim, hidden_size = self.hidden_dim, num_layers = self.num_layers, batch_first = True)\n",
    "        \n",
    "        #self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim), requires_grad = False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        \n",
    "       # out = self.fc(self.hidden[-1].view(-1,self.hidden_dim))\n",
    "        \n",
    "        return lstm_out#self.hidden[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(input_dim = input_dim, \n",
    "                embedding_dim = embedding_dim,\n",
    "                seq_length = seq_length,\n",
    "                hidden_dim = hidden_dim,\n",
    "                batch_size = batch_size, \n",
    "                output_dim = output_dim, \n",
    "                num_layers = num_layers)\n",
    "\n",
    "# model  = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Drives Done: 10 Accuracy: 53.0\n",
      "Epoch: 0 Drives Done: 20 Accuracy: 53.0\n",
      "Epoch: 0 Drives Done: 30 Accuracy: 62.0\n",
      "Epoch: 0 Drives Done: 40 Accuracy: 73.0\n",
      "Epoch: 0 Drives Done: 50 Accuracy: 81.0\n",
      "Epoch: 0 Drives Done: 60 Accuracy: 73.0\n",
      "Epoch: 0 Drives Done: 70 Accuracy: 77.0\n",
      "Epoch: 0 Drives Done: 80 Accuracy: 85.0\n",
      "Epoch: 0 Drives Done: 90 Accuracy: 85.0\n",
      "Epoch: 0 Drives Done: 100 Accuracy: 88.0\n",
      "Epoch: 0 Drives Done: 110 Accuracy: 86.0\n",
      "Epoch: 0 Drives Done: 120 Accuracy: 87.0\n",
      "Epoch: 0 Drives Done: 130 Accuracy: 79.0\n",
      "Epoch: 0 Drives Done: 140 Accuracy: 80.0\n",
      "Epoch: 0 Drives Done: 150 Accuracy: 83.0\n",
      "Epoch: 0 Drives Done: 160 Accuracy: 82.0\n",
      "Epoch: 0 Drives Done: 170 Accuracy: 84.0\n",
      "Epoch: 0 Drives Done: 180 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 190 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 200 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 210 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 220 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 230 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 240 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 250 Accuracy: 88.0\n",
      "Epoch: 0 Drives Done: 260 Accuracy: 88.0\n",
      "Epoch: 0 Drives Done: 270 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 280 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 290 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 300 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 310 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 320 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 330 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 340 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 350 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 360 Accuracy: 88.0\n",
      "Epoch: 0 Drives Done: 370 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 380 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 390 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 400 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 410 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 420 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 430 Accuracy: 86.0\n",
      "Epoch: 0 Drives Done: 440 Accuracy: 88.0\n",
      "Epoch: 0 Drives Done: 450 Accuracy: 89.0\n",
      "Epoch: 0 Drives Done: 460 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 470 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 480 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 490 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 500 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 510 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 520 Accuracy: 94.0\n",
      "Epoch: 0 Drives Done: 530 Accuracy: 95.0\n",
      "Epoch: 0 Drives Done: 540 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 550 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 560 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 570 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 580 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 590 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 600 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 610 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 620 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 630 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 640 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 650 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 660 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 670 Accuracy: 92.0\n",
      "Epoch: 0 Drives Done: 680 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 690 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 700 Accuracy: 90.0\n",
      "Epoch: 0 Drives Done: 710 Accuracy: 91.0\n",
      "Epoch: 0 Drives Done: 720 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 730 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 740 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 750 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 760 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 770 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 780 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 790 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 800 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 810 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 820 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 830 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 840 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 850 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 860 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 870 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 880 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 890 Accuracy: 93.0\n",
      "Epoch: 0 Drives Done: 900 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 10 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 20 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 30 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 40 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 50 Accuracy: 93.0\n",
      "Epoch: 1 Drives Done: 60 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 70 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 80 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 90 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 100 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 110 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 120 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 130 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 140 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 150 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 160 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 170 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 180 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 190 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 200 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 210 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 220 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 230 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 240 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 250 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 260 Accuracy: 99.0\n",
      "Epoch: 1 Drives Done: 270 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 280 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 290 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 300 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 310 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 320 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 330 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 340 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 350 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 360 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 370 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 380 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 390 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 400 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 410 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 420 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 430 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 440 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 450 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 460 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 470 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 480 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 490 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 500 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 510 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 520 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 530 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 540 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 550 Accuracy: 96.0\n",
      "Epoch: 1 Drives Done: 560 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 570 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 580 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 590 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 600 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 610 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 620 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 630 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 640 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 650 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 660 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 670 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 680 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 690 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 700 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 710 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 720 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 730 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 740 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 750 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 760 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 770 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 780 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 790 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 800 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 810 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 820 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 830 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 840 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 850 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 860 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 870 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 880 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 890 Accuracy: 98.0\n",
      "Epoch: 1 Drives Done: 900 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 10 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 20 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 30 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 40 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 50 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 60 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 70 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 80 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 90 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 100 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 110 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 120 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 130 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 140 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 150 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 160 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 170 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 180 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 190 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 200 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 210 Accuracy: 99.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Drives Done: 220 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 230 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 240 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 250 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 260 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 270 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 280 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 290 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 300 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 310 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 320 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 330 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 340 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 350 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 360 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 370 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 380 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 390 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 400 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 410 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 420 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 430 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 440 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 450 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 460 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 470 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 480 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 490 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 500 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 510 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 520 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 530 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 540 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 550 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 560 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 570 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 580 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 590 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 600 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 610 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 620 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 630 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 640 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 650 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 660 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 670 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 680 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 690 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 700 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 710 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 720 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 730 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 740 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 750 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 760 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 770 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 780 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 790 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 800 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 810 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 820 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 830 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 840 Accuracy: 99.0\n",
      "Epoch: 2 Drives Done: 850 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 860 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 870 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 880 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 890 Accuracy: 98.0\n",
      "Epoch: 2 Drives Done: 900 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 10 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 20 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 30 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 40 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 50 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 60 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 70 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 80 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 90 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 100 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 110 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 120 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 130 Accuracy: 99.0\n",
      "Epoch: 3 Drives Done: 140 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 150 Accuracy: 98.0\n",
      "Epoch: 3 Drives Done: 160 Accuracy: 97.0\n",
      "Epoch: 3 Drives Done: 170 Accuracy: 97.0\n",
      "Epoch: 3 Drives Done: 180 Accuracy: 97.0\n",
      "Epoch: 3 Drives Done: 190 Accuracy: 97.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ba85d30257ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#training through dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdrive_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdrive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_numpy_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets_numpy_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#Rescale data so that max is <= 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-ba85d30257ce>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_features_numpy_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_features_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_targets_numpy_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_targets_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#training through dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tables/array.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# First, try with a regular selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mstartl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;31m# Then, try with a point-wise selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tables/array.py\u001b[0m in \u001b[0;36m_read_slice\u001b[0;34m(self, startl, stopl, stepl, shape)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;31m# Arrays that have non-zero dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_read_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;31m# For zero-shaped arrays, return the scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnparr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tables\n",
    "num_epochs = 10\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "train_features_numpy_filename = \"data/TrainFeatures800K.h5\"\n",
    "train_targets_numpy_filename = \"data/TrainLabels800K.h5\"\n",
    "np.random.seed(10)\n",
    "train_features_file = tables.open_file(train_features_numpy_filename, mode='r')\n",
    "train_targets_file = tables.open_file(train_targets_numpy_filename, mode='r')\n",
    "index_list = np.arange(5192)\n",
    "np.random.shuffle(index_list)\n",
    "train_sample_size = 1000\n",
    "index_list = index_list[:train_sample_size]\n",
    "train_len = int(.9*train_sample_size)\n",
    "# max_fea = 0\n",
    "# min_fea = 0\n",
    "\n",
    "# for i in range(0, train_sample_size, 100):\n",
    "#     data_to_train = train_features_file.root.data[i:i+100] #100X800000\n",
    "#     max_fea = max(max_fea, np.max(data_to_train))\n",
    "#     min_fea = min(min_fea, np.min(data_to_train))\n",
    "# scale = (1 - (-1))/(max_fea - min_fea) \n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "    train_features_numpy_tr = (train_features_file.root.data[i] for i in index_list[:train_len])\n",
    "    train_targets_numpy_tr = (train_targets_file.root.data[i] for i in index_list[:train_len])\n",
    "    #training through dataset\n",
    "    drive_num = 0\n",
    "    for (drive, label) in zip(train_features_numpy_tr, train_targets_numpy_tr):   \n",
    "        model.hidden = model.init_hidden()\n",
    "        #Rescale data so that max is <= 1\n",
    "        #drive = drive*scale + (-1) - min_fea*scale \n",
    "        batching_obj = DoBatches(length=input_dim*seq_length, stride=stride,signal = drive.reshape(800000,1))\n",
    "        \n",
    "        this_drive_batches = np.zeros((input_dim*seq_length,1))\n",
    "        this_batch, is_done = batching_obj.getNextBatch()\n",
    "        num_batches = 0\n",
    "        while(this_batch is not None and this_batch.shape[0] > 0 and not is_done):\n",
    "            num_batches += 1\n",
    "            this_drive_batches = np.append(this_drive_batches, this_batch, axis = 1)\n",
    "            this_batch, is_done = batching_obj.getNextBatch()\n",
    "        this_drive_batches = np.delete(this_drive_batches, (0), axis=1) \n",
    "        \n",
    "        features_curr_drive_batch_tensor = torch.from_numpy(np.array(this_drive_batches)).type(torch.FloatTensor)\n",
    "        \n",
    "        train = Variable(features_curr_drive_batch_tensor.view((batch_size,seq_length,input_dim)))\n",
    "        \n",
    "#        labels = np.repeat(int(label),num_batches)\n",
    "        labels = np.full((num_batches*seq_length,),int(label))\n",
    "        targets_curr_drive_batch_tensor = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "        targets = Variable(targets_curr_drive_batch_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train)  \n",
    "        outputs = outputs.reshape(-1,2)\n",
    "        loss = criterion(outputs, targets)\n",
    "        #s = torch.sum(model.lstm.weight_hh_l0.chunk(3,0)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        drive_num += 1\n",
    "\n",
    "        if (drive_num%10 == 0):\n",
    "            #validation\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            train_features_numpy_val = (train_features_file.root.data[i] for i in index_list[train_len:])\n",
    "            train_targets_numpy_val = (train_targets_file.root.data[i] for i in index_list[train_len:])\n",
    "            for test_drive, test_label in zip(train_features_numpy_val, train_targets_numpy_val):        \n",
    "                #test_drive = test_drive*scale + (-1) - min_fea *scale \n",
    "                total += 1\n",
    "                batching_obj = DoBatches(length=input_dim*seq_length, stride=stride,signal = test_drive.reshape(800000,1))\n",
    "\n",
    "                this_drive_batches = np.zeros((input_dim*seq_length,1))\n",
    "\n",
    "                this_batch, is_done = batching_obj.getNextBatch()\n",
    "                num_batches = 0\n",
    "                while(this_batch is not None and this_batch.shape[0] > 0 and not is_done):\n",
    "                    num_batches += 1\n",
    "                    this_drive_batches = np.append(this_drive_batches, this_batch, axis = 1)\n",
    "                    this_batch, is_done = batching_obj.getNextBatch()\n",
    "\n",
    "\n",
    "                this_drive_batches = np.delete(this_drive_batches, (0), axis=1) \n",
    "\n",
    "                features_curr_drive_batch_tensor = torch.from_numpy(np.array(this_drive_batches)).type(torch.FloatTensor)\n",
    "\n",
    "                test = Variable(features_curr_drive_batch_tensor.view((batch_size,seq_length,input_dim)))\n",
    "\n",
    "                test_labels = np.repeat(int(test_label),num_batches)\n",
    "                \n",
    "                #test_labels = np.full((num_batches*seq_length,),int(label))\n",
    "        \n",
    "                outputs = model(test)\n",
    "                \n",
    "                #outputs = outputs.reshape(-1, num_classes)\n",
    "                outputs = outputs[:, -1]\n",
    "                predicted = torch.max(outputs.data, 1)[1].data.numpy()\n",
    "                correct += (predicted == test_labels).sum()\n",
    "            accuracy = 100*correct/float(total*batch_size)\n",
    "            print(\"Epoch: {} Drives Done: {} Accuracy: {}\".format(epoch, drive_num, accuracy))\n",
    "    \n",
    "    #loss_list.append(loss.data)\n",
    "    #accuracy_list.append(accuracy)\n",
    "\n",
    "    #print('Epoch: {} Loss: {} Accuracy: {} %'.format(epoch+1, loss.data.item(), accuracy))\n",
    "\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.title('Final Loss curve')\n",
    "# plt.plot(loss_list)\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.title('Final Validation Curve')\n",
    "# plt.plot(accuracy_list)\n",
    "train_features_file.close()\n",
    "train_targets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset dimensions\n",
      "Shape of Features Dataset\n",
      "torch.Size([8, 800000])\n",
      "Shape of Targets Dataset\n",
      "torch.Size([8])\n",
      "Batch Size: 8\n"
     ]
    }
   ],
   "source": [
    "test_features = torch.from_numpy(test_features_numpy).type(torch.FloatTensor)\n",
    "test_targets = torch.from_numpy(test_targets_numpy).type(torch.FloatTensor)\n",
    "test_totalDataset = torch.utils.data.TensorDataset(test_features,test_targets)\n",
    "batch_size_test = test_features_numpy.shape[0]\n",
    "test_loader = torch.utils.data.DataLoader(test_totalDataset, batch_size = batch_size_test, shuffle = True)\n",
    "\n",
    "\n",
    "print('Testing dataset dimensions')\n",
    "print('Shape of Features Dataset')\n",
    "print(test_features.size())\n",
    "print('Shape of Targets Dataset')\n",
    "print(test_targets.size())\n",
    "print(\"Batch Size: {}\".format(batch_size_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "accuracy_list = []\n",
    "f1_score_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "y_pred =[]\n",
    "y_true = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for test_drive, test_label in zip(test_features_numpy, test_targets_numpy):            \n",
    "        total += 1\n",
    "        batching_obj = DoBatches(length=input_dim*seq_length, stride=stride,signal = test_drive.reshape(800000,1))\n",
    "\n",
    "        this_drive_batches = np.zeros((input_dim*seq_length,1))\n",
    "\n",
    "        this_batch, is_done = batching_obj.getNextBatch()\n",
    "        num_batches = 0\n",
    "        while(this_batch is not None and this_batch.shape[0] > 0 and not is_done):\n",
    "            num_batches += 1\n",
    "            this_drive_batches = np.append(this_drive_batches, this_batch, axis = 1)\n",
    "            this_batch, is_done = batching_obj.getNextBatch()\n",
    "\n",
    "\n",
    "        this_drive_batches = np.delete(this_drive_batches, (0), axis=1) \n",
    "\n",
    "        features_curr_drive_batch_tensor = torch.from_numpy(np.array(this_drive_batches)).type(torch.FloatTensor)\n",
    "\n",
    "        test = Variable(features_curr_drive_batch_tensor.view((batch_size,seq_length,input_dim)))\n",
    "\n",
    "        test_labels = np.repeat(int(test_label),num_batches)\n",
    "\n",
    "        #test_labels = np.full((num_batches*seq_length,),int(label))\n",
    "\n",
    "        outputs = model(test)\n",
    "\n",
    "        #outputs = outputs.reshape(-1, num_classes)\n",
    "        outputs = outputs[:, -1]\n",
    "        predicted = torch.max(outputs.data, 1)[1].data.numpy()\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    accuracy = 100*correct/float(total*batch_size)\n",
    "    print(accuracy)\n",
    "#     for i, data in enumerate(test_loader):\n",
    "#             samples, labels = data\n",
    "#             samples = Variable(samples.view(batch_size_test,1,-1))\n",
    "            \n",
    "#             model.eval()\n",
    "            \n",
    "#             outputs = model(samples)\n",
    "#             outputs = outputs.view(batch_size_test, n_classes)\n",
    "            \n",
    "#             predictions = torch.argmax(outputs, 1)\n",
    "#             targets = labels\n",
    "            \n",
    "#             y_pred.extend(predictions)\n",
    "#             y_true.extend(targets)\n",
    "\n",
    "\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# print(conf_matrix)\n",
    "# print(\"Accuracy Score: {}\".format(accuracy_score(y_true, y_pred) * 100))\n",
    "# print(\"F1 Score: {}\". format(f1_score(y_true, y_pred) * 100))\n",
    "# print(\"Precision Score: {}\".format(precision_score(y_true, y_pred) * 100))\n",
    "# print(\"Recall Score: {}\".format(recall_score(y_true, y_pred)* 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 5, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(10):\n",
    "        ax[i%2][i%5].plot(np.arange(50000),vw_features_numpy[i,:])\n",
    "plt.title(\"Volkswagen Lane 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 5, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(10):\n",
    "        ax[i%2][i%5].plot(np.arange(50000),features_numpy[i,:])\n",
    "plt.title(\"Subaru Lane 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
